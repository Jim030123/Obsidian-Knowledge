---
done: false
remark:
---


>[!note] Definition
>The problem of achieving **agreement between our true preferences and the objective** we put into the machine
> 我们的真实偏好和我们输入机器的**目标之间实现一致的问题**
> 
>The **values or objectives** put into the **machine must be aligned with those of the human**.
>**机器的价值观或目标**必须与**人的价值观或目标一致**。

---
# Key Concept
If **developing an AI system in the lab** or in **a simulator**, there is an **easy fix for an incorrectly specified objective**:
如果在**实验室**或**模拟器中开发人工智能系统**，**可以很容易地解决目标**指定错误的问题：

- Reset the system 重置系统
- Fix the objective 确定目标
- Try again 再试一次

>[!danger] A system deployed with an incorrect objective will have negative consequences 目标不正确的系统部署会产生负面影响
>
>More intelligent the system, the more negative the consequences.
>系统越智能，负面影响就越大。

It is impossible to anticipate all the ways in which a **machine pursing a fixed objective might misbehave**.
我们不可能预料到一台追求固定目标的**机器会以何种方式做出错误行为。

We **don't want machines that are intelligent in the sense of pursuing their objectives**, but then need a new formulation - one in which the **machine is pursuing our objectives, but is necessarily uncertain as to what they are**.
我们不希望**机器在追求目标的意义上是智能的，而是需要一种新的表述--在这种表述中，**机器追求我们的目标，但必然不确定这些目标是什么**。

When a **machine knows that it doesn't know the complete objective**, it has an **incentive to act cautiously, to ask permission**, to **learn more about our preferences through observation and to defer to human control**.^[Ultimately, want agents that are provably beneficial to humans. 归根结底，我们想要的是对人类有益的代理。]
当**机器知道它不知道完整的目标**时，它就有**动力谨慎行事，征求许可**，**通过观察更多地了解我们的偏好，并服从人类的控制**。

---